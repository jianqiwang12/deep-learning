{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 60000\n",
      "The number of validation data: 10000\n"
     ]
    }
   ],
   "source": [
    "trans = torchvision.transforms.ToTensor()\n",
    "train_data = datasets.FashionMNIST(root='../data', train=True, download=False, transform=trans)\n",
    "val_data = datasets.FashionMNIST(root='../data', train=False, download=False, transform=trans)\n",
    "print('The number of training data:', len(train_data))\n",
    "print('The number of validation data:', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
    "for X, y in val_dataloader:\n",
    "    print('Shape of X:', X.shape)\n",
    "    print('Shape of y:', y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.pool1(F.relu(self.conv1(X)))\n",
    "        X = self.pool2(F.relu(self.conv2(X)))\n",
    "        X = self.flatten(X)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, net, loss, optim):\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        if batch%100 == 0:\n",
    "            running_loss = l.item()\n",
    "            current = batch * len(X)\n",
    "            print('Train Loss: %.4f, [%d//%d]' % (running_loss, current, len(dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, net, loss):\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss = 0.0\n",
    "    acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            val_loss += loss(y_hat, y).item()\n",
    "            acc += (y_hat.argmax(1)==y).type(torch.float32).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    acc /= len(dataloader.dataset)\n",
    "    print('Val Acc: %.4f, Val Avg Loss: %.4f' % (acc, val_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "epoch: 1\n",
      "Train Loss: 2.3062, [0//60000]\n",
      "Train Loss: 2.2977, [6400//60000]\n",
      "Train Loss: 0.8749, [12800//60000]\n",
      "Train Loss: 0.9043, [19200//60000]\n",
      "Train Loss: 0.7966, [25600//60000]\n",
      "Train Loss: 0.7048, [32000//60000]\n",
      "Train Loss: 0.6319, [38400//60000]\n",
      "Train Loss: 0.6037, [44800//60000]\n",
      "Train Loss: 0.5233, [51200//60000]\n",
      "Train Loss: 0.6208, [57600//60000]\n",
      "Val Acc: 0.7861, Val Avg Loss: 0.5683\n",
      "epoch: 2\n",
      "Train Loss: 0.4795, [0//60000]\n",
      "Train Loss: 0.4725, [6400//60000]\n",
      "Train Loss: 0.3745, [12800//60000]\n",
      "Train Loss: 0.5468, [19200//60000]\n",
      "Train Loss: 0.5409, [25600//60000]\n",
      "Train Loss: 0.4852, [32000//60000]\n",
      "Train Loss: 0.4862, [38400//60000]\n",
      "Train Loss: 0.5730, [44800//60000]\n",
      "Train Loss: 0.4819, [51200//60000]\n",
      "Train Loss: 0.5447, [57600//60000]\n",
      "Val Acc: 0.8138, Val Avg Loss: 0.4837\n",
      "epoch: 3\n",
      "Train Loss: 0.4170, [0//60000]\n",
      "Train Loss: 0.4129, [6400//60000]\n",
      "Train Loss: 0.3218, [12800//60000]\n",
      "Train Loss: 0.4893, [19200//60000]\n",
      "Train Loss: 0.5016, [25600//60000]\n",
      "Train Loss: 0.4403, [32000//60000]\n",
      "Train Loss: 0.3970, [38400//60000]\n",
      "Train Loss: 0.4998, [44800//60000]\n",
      "Train Loss: 0.4921, [51200//60000]\n",
      "Train Loss: 0.4641, [57600//60000]\n",
      "Val Acc: 0.8364, Val Avg Loss: 0.4329\n",
      "epoch: 4\n",
      "Train Loss: 0.3529, [0//60000]\n",
      "Train Loss: 0.3437, [6400//60000]\n",
      "Train Loss: 0.2810, [12800//60000]\n",
      "Train Loss: 0.4428, [19200//60000]\n",
      "Train Loss: 0.4388, [25600//60000]\n",
      "Train Loss: 0.3901, [32000//60000]\n",
      "Train Loss: 0.3412, [38400//60000]\n",
      "Train Loss: 0.4936, [44800//60000]\n",
      "Train Loss: 0.4641, [51200//60000]\n",
      "Train Loss: 0.4195, [57600//60000]\n",
      "Val Acc: 0.8442, Val Avg Loss: 0.4111\n",
      "epoch: 5\n",
      "Train Loss: 0.3125, [0//60000]\n",
      "Train Loss: 0.2946, [6400//60000]\n",
      "Train Loss: 0.2626, [12800//60000]\n",
      "Train Loss: 0.3857, [19200//60000]\n",
      "Train Loss: 0.3915, [25600//60000]\n",
      "Train Loss: 0.3713, [32000//60000]\n",
      "Train Loss: 0.3138, [38400//60000]\n",
      "Train Loss: 0.4879, [44800//60000]\n",
      "Train Loss: 0.4304, [51200//60000]\n",
      "Train Loss: 0.3699, [57600//60000]\n",
      "Val Acc: 0.8577, Val Avg Loss: 0.3802\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "print('training on', device)\n",
    "for epoch in range(EPOCHS):\n",
    "    print('epoch: %d' % (epoch+1))\n",
    "    train(train_dataloader, net, loss, optim)\n",
    "    test(val_dataloader, net, loss)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ebc51c9b13dc99464744e6a2bfb4e0ab09dbecf191f9790c30f90ddc17ba378"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
